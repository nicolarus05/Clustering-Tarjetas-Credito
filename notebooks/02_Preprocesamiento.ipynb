{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0b189a",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Preprocesamiento de Datos\n",
    "## Segmentaci√≥n de Clientes con Tarjetas de Cr√©dito\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Informaci√≥n del Notebook\n",
    "\n",
    "- **Objetivo**: Limpiar y preparar los datos para el clustering\n",
    "- **Acciones**: Manejo de nulos, normalizaci√≥n, detecci√≥n de outliers\n",
    "- **Autor**: [Tu Nombre]\n",
    "- **Fecha**: Enero 2026\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Contenido\n",
    "\n",
    "1. Carga de datos\n",
    "2. Manejo de valores nulos\n",
    "3. Eliminaci√≥n de duplicados\n",
    "4. An√°lisis y tratamiento de outliers\n",
    "5. Normalizaci√≥n de datos\n",
    "6. Reducci√≥n de dimensionalidad (PCA)\n",
    "7. Guardado de datos procesados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librer√≠as para manipulaci√≥n de datos, preprocesamiento y visualizaci√≥n\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a78956",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÅ 1. Carga del Dataset Original\n",
    "\n",
    "### Cargar datos desde el archivo CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1804f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset original que analizamos en el notebook anterior\n",
    "# El archivo debe estar en la carpeta datos/\n",
    "df_original = pd.read_csv('../datos/CC_GENERAL.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä DATASET CARGADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Filas: {df_original.shape[0]:,}\")\n",
    "print(f\"Columnas: {df_original.shape[1]}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218a3df",
   "metadata": {},
   "source": [
    "### Crear copia para trabajar sin modificar el original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una copia del dataframe para no modificar los datos originales\n",
    "# Esto nos permite volver atr√°s si es necesario\n",
    "df = df_original.copy()\n",
    "\n",
    "print(f\"‚úÖ Copia de trabajo creada: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd014c",
   "metadata": {},
   "source": [
    "---\n",
    "## üóëÔ∏è 2. Eliminaci√≥n de Columnas Innecesarias\n",
    "\n",
    "### Eliminar la columna CUST_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna CUST_ID porque:\n",
    "# 1. Es un identificador √∫nico que no aporta informaci√≥n para clustering\n",
    "# 2. No tiene valor predictivo\n",
    "# 3. Puede sesgar los algoritmos de ML\n",
    "if 'CUST_ID' in df.columns:\n",
    "    df = df.drop('CUST_ID', axis=1)\n",
    "    print(\"‚úÖ Columna CUST_ID eliminada\")\n",
    "    print(f\"   Nuevas dimensiones: {df.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUST_ID no encontrada en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f59386",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç 3. An√°lisis de Valores Nulos\n",
    "\n",
    "### Identificar columnas con valores nulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902eb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos qu√© columnas tienen valores nulos y en qu√© cantidad\n",
    "# Esto nos ayuda a decidir la estrategia de imputaci√≥n\n",
    "nulos_total = df.isnull().sum()\n",
    "nulos_df = pd.DataFrame({\n",
    "    'Columna': nulos_total.index,\n",
    "    'Nulos': nulos_total.values,\n",
    "    '% Nulos': (nulos_total.values / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "nulos_df = nulos_df[nulos_df['Nulos'] > 0].sort_values('Nulos', ascending=False)\n",
    "\n",
    "print(\"üîé AN√ÅLISIS DE VALORES NULOS\")\n",
    "print(\"=\"*70)\n",
    "if len(nulos_df) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è Columnas con valores nulos:\")\n",
    "    print(nulos_df.to_string(index=False))\n",
    "    print(f\"\\nTotal de valores nulos: {df.isnull().sum().sum():,}\")\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores nulos\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956baea5",
   "metadata": {},
   "source": [
    "### Imputar valores nulos con la mediana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e714a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputamos los valores nulos usando la mediana porque:\n",
    "# 1. La mediana es robusta a outliers (nuestro dataset tiene muchos)\n",
    "# 2. Es mejor que la media para distribuciones asim√©tricas\n",
    "# 3. Preserva la distribuci√≥n original de los datos\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    columnas = df.columns\n",
    "    df_imputado = pd.DataFrame(\n",
    "        imputer.fit_transform(df),\n",
    "        columns=columnas\n",
    "    )\n",
    "    df = df_imputado\n",
    "    print(\"‚úÖ Valores nulos imputados con la mediana\")\n",
    "    print(f\"   Verificaci√≥n: {df.isnull().sum().sum()} nulos restantes\")\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores nulos que imputar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d56f7",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ 4. Eliminaci√≥n de Duplicados\n",
    "\n",
    "### Identificar y eliminar registros duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos y eliminamos registros duplicados\n",
    "# Los duplicados pueden sesgar el clustering al dar m√°s peso a ciertos patrones\n",
    "duplicados_antes = df.duplicated().sum()\n",
    "\n",
    "if duplicados_antes > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    duplicados_despues = df.duplicated().sum()\n",
    "    \n",
    "    print(\"üîÑ ELIMINACI√ìN DE DUPLICADOS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Duplicados encontrados: {duplicados_antes}\")\n",
    "    print(f\"Registros eliminados: {duplicados_antes - duplicados_despues}\")\n",
    "    print(f\"Duplicados restantes: {duplicados_despues}\")\n",
    "    print(f\"Nuevas dimensiones: {df.shape}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"‚úÖ No hay registros duplicados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c9895",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 5. An√°lisis de Outliers\n",
    "\n",
    "### Detectar outliers usando el m√©todo IQR (Rango Intercuart√≠lico)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para detectar outliers usando el m√©todo IQR\n",
    "# IQR = Q3 - Q1 (rango entre percentil 75 y 25)\n",
    "# Outliers: valores < Q1 - 1.5*IQR o > Q3 + 1.5*IQR\n",
    "def detectar_outliers_iqr(df, columna):\n",
    "    Q1 = df[columna].quantile(0.25)\n",
    "    Q3 = df[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)]\n",
    "    \n",
    "    return len(outliers), limite_inferior, limite_superior\n",
    "\n",
    "# Aplicamos la detecci√≥n a todas las columnas num√©ricas\n",
    "print(\"üîç DETECCI√ìN DE OUTLIERS (M√©todo IQR)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "outliers_resumen = []\n",
    "for col in df.columns:\n",
    "    n_outliers, lim_inf, lim_sup = detectar_outliers_iqr(df, col)\n",
    "    porcentaje = (n_outliers / len(df)) * 100\n",
    "    \n",
    "    outliers_resumen.append({\n",
    "        'Variable': col,\n",
    "        'N_Outliers': n_outliers,\n",
    "        '%_Outliers': round(porcentaje, 2)\n",
    "    })\n",
    "\n",
    "df_outliers = pd.DataFrame(outliers_resumen)\n",
    "df_outliers = df_outliers.sort_values('N_Outliers', ascending=False)\n",
    "\n",
    "print(df_outliers.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8b3a3",
   "metadata": {},
   "source": [
    "### Visualizar outliers en variables clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25163fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos boxplots para visualizar los outliers en las variables m√°s importantes\n",
    "# Esto nos ayuda a decidir si los mantenemos o eliminamos\n",
    "variables_clave = ['BALANCE', 'PURCHASES', 'CREDIT_LIMIT', 'PAYMENTS', 'CASH_ADVANCE']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for idx, var in enumerate(variables_clave):\n",
    "    # Creamos un boxplot para cada variable\n",
    "    axes[idx].boxplot(df[var].dropna(), vert=True)\n",
    "    axes[idx].set_title(f'{var}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Valor')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculamos cantidad de outliers\n",
    "    n_out, _, _ = detectar_outliers_iqr(df, var)\n",
    "    axes[idx].text(0.5, 0.98, f'Outliers: {n_out}', \n",
    "                   transform=axes[idx].transAxes,\n",
    "                   fontsize=9, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Outliers en Variables Clave', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96c7e1",
   "metadata": {},
   "source": [
    "### Decisi√≥n sobre outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISI√ìN: NO eliminamos outliers porque:\n",
    "# 1. En datos de tarjetas de cr√©dito, los outliers pueden representar segmentos reales\n",
    "#    (clientes VIP, usuarios frecuentes, etc.)\n",
    "# 2. El clustering puede identificar estos grupos como clusters separados\n",
    "# 3. La normalizaci√≥n reducir√° su impacto\n",
    "# 4. Eliminarlos podr√≠a perder informaci√≥n valiosa de negocio\n",
    "\n",
    "print(\"‚ö†Ô∏è DECISI√ìN SOBRE OUTLIERS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Los outliers NO ser√°n eliminados porque:\")\n",
    "print(\"  1. Pueden representar segmentos de clientes reales\")\n",
    "print(\"  2. Son valiosos para identificar clusters especiales (VIP, etc.)\")\n",
    "print(\"  3. La normalizaci√≥n mitigar√° su impacto\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6bcd5",
   "metadata": {},
   "source": [
    "---\n",
    "## üìè 6. Normalizaci√≥n de Datos\n",
    "\n",
    "### Aplicar StandardScaler para normalizar todas las variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos los datos usando StandardScaler\n",
    "# Esto transforma cada variable a media=0 y desviaci√≥n est√°ndar=1\n",
    "# Es CRUCIAL para clustering porque:\n",
    "# 1. Las variables tienen escalas muy diferentes (TENURE: 0-12 vs BALANCE: 0-20,000)\n",
    "# 2. KMeans es sensible a las escalas\n",
    "# 3. Evita que variables con valores grandes dominen el clustering\n",
    "scaler = StandardScaler()\n",
    "datos_normalizados = scaler.fit_transform(df)\n",
    "\n",
    "df_normalizado = pd.DataFrame(\n",
    "    datos_normalizados,\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DATOS NORMALIZADOS CON STANDARDSCALER\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dimensiones: {df_normalizado.shape}\")\n",
    "print(f\"\\nVerificaci√≥n de normalizaci√≥n (debe ser ‚âà0 y ‚âà1):\")\n",
    "print(f\"  Media de todas las variables: {df_normalizado.mean().mean():.6f}\")\n",
    "print(f\"  Desviaci√≥n est√°ndar promedio: {df_normalizado.std().mean():.6f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859e510",
   "metadata": {},
   "source": [
    "### Comparar datos antes y despu√©s de normalizaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos una variable antes y despu√©s de normalizar\n",
    "# Esto nos permite visualizar el efecto de la normalizaci√≥n\n",
    "variable_ejemplo = 'BALANCE'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Antes de normalizar\n",
    "axes[0].hist(df[variable_ejemplo], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'{variable_ejemplo} - ANTES de Normalizar', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Valor Original')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Despu√©s de normalizar\n",
    "axes[1].hist(df_normalizado[variable_ejemplo], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(f'{variable_ejemplo} - DESPU√âS de Normalizar', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Valor Normalizado (z-score)')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Efecto de la Normalizaci√≥n', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Variable {variable_ejemplo}:\")\n",
    "print(f\"   Antes  ‚Üí Media: {df[variable_ejemplo].mean():.2f}, Std: {df[variable_ejemplo].std():.2f}\")\n",
    "print(f\"   Despu√©s ‚Üí Media: {df_normalizado[variable_ejemplo].mean():.6f}, Std: {df_normalizado[variable_ejemplo].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db50574",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ 7. Reducci√≥n de Dimensionalidad (PCA)\n",
    "\n",
    "### Aplicar PCA para visualizaci√≥n y an√°lisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a16b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos PCA (Principal Component Analysis) para:\n",
    "# 1. Reducir dimensionalidad (17 variables ‚Üí 2-3 componentes principales)\n",
    "# 2. Facilitar la visualizaci√≥n de clusters\n",
    "# 3. Capturar la mayor varianza posible con menos variables\n",
    "# 4. Reducir ruido y redundancia\n",
    "\n",
    "# Primero probamos con todos los componentes para ver varianza explicada\n",
    "pca_completo = PCA()\n",
    "pca_completo.fit(df_normalizado)\n",
    "\n",
    "varianza_explicada = pca_completo.explained_variance_ratio_\n",
    "varianza_acumulada = np.cumsum(varianza_explicada)\n",
    "\n",
    "print(\"üìä AN√ÅLISIS PCA - VARIANZA EXPLICADA\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(10, len(varianza_explicada))):\n",
    "    print(f\"PC{i+1}: {varianza_explicada[i]*100:6.2f}% | Acumulada: {varianza_acumulada[i]*100:6.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd301e",
   "metadata": {},
   "source": [
    "### Visualizar varianza explicada (Scree Plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e706bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Scree Plot para decidir cu√°ntos componentes usar\n",
    "# Este gr√°fico muestra cu√°nta varianza explica cada componente\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Gr√°fico 1: Varianza por componente\n",
    "axes[0].plot(range(1, len(varianza_explicada)+1), \n",
    "             varianza_explicada * 100, \n",
    "             marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].set_title('Scree Plot - Varianza por Componente', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Componente Principal')\n",
    "axes[0].set_ylabel('Varianza Explicada (%)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=10, color='red', linestyle='--', label='10% umbral')\n",
    "axes[0].legend()\n",
    "\n",
    "# Gr√°fico 2: Varianza acumulada\n",
    "axes[1].plot(range(1, len(varianza_acumulada)+1), \n",
    "             varianza_acumulada * 100, \n",
    "             marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_title('Varianza Explicada Acumulada', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('N√∫mero de Componentes')\n",
    "axes[1].set_ylabel('Varianza Acumulada (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=80, color='red', linestyle='--', label='80% objetivo')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostramos cu√°ntos componentes necesitamos para 80% y 90% de varianza\n",
    "for umbral in [0.80, 0.90]:\n",
    "    n_comp = np.argmax(varianza_acumulada >= umbral) + 1\n",
    "    print(f\"‚úÖ Para {umbral*100:.0f}% varianza ‚Üí {n_comp} componentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcda8c",
   "metadata": {},
   "source": [
    "### Aplicar PCA con componentes √≥ptimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos PCA con el n√∫mero √≥ptimo de componentes\n",
    "# Usaremos los componentes que explican al menos 80% de la varianza\n",
    "n_componentes_optimo = np.argmax(varianza_acumulada >= 0.80) + 1\n",
    "\n",
    "pca = PCA(n_components=n_componentes_optimo)\n",
    "datos_pca = pca.fit_transform(df_normalizado)\n",
    "\n",
    "df_pca = pd.DataFrame(\n",
    "    datos_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(n_componentes_optimo)]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ PCA APLICADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Componentes principales: {n_componentes_optimo}\")\n",
    "print(f\"Varianza explicada total: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "print(f\"Dimensiones reducidas: {df.shape[1]} ‚Üí {df_pca.shape[1]} variables\")\n",
    "print(f\"Forma de datos PCA: {df_pca.shape}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a4b8d",
   "metadata": {},
   "source": [
    "### Visualizar contribuci√≥n de variables a componentes principales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos qu√© variables originales contribuyen m√°s a cada componente principal\n",
    "# Esto nos ayuda a interpretar el significado de cada PC\n",
    "componentes_df = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_componentes_optimo)],\n",
    "    index=df.columns\n",
    ")\n",
    "\n",
    "# Mostramos las 5 variables m√°s importantes para PC1 y PC2\n",
    "print(\"\\nüìä CONTRIBUCI√ìN DE VARIABLES A COMPONENTES PRINCIPALES\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for pc in ['PC1', 'PC2']:\n",
    "    print(f\"\\n{pc} - Top 5 variables m√°s influyentes:\")\n",
    "    top_vars = componentes_df[pc].abs().sort_values(ascending=False).head(5)\n",
    "    for var, peso in top_vars.items():\n",
    "        print(f\"  {var:35s}: {componentes_df.loc[var, pc]:7.3f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f908da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de contribuci√≥n de variables a componentes\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(\n",
    "    componentes_df[['PC1', 'PC2', 'PC3']],\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Peso del componente'}\n",
    ")\n",
    "plt.title('Contribuci√≥n de Variables a los Primeros 3 Componentes Principales', \n",
    "          fontsize=13, fontweight='bold', pad=15)\n",
    "plt.xlabel('Componentes Principales')\n",
    "plt.ylabel('Variables Originales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923a2ad",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ 8. Guardar Datos Procesados\n",
    "\n",
    "### Exportar datos limpios y normalizados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202590eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los datos procesados en diferentes formatos para el siguiente notebook\n",
    "# 1. Datos normalizados (para clustering)\n",
    "# 2. Datos PCA (para visualizaci√≥n)\n",
    "# 3. Objeto scaler (para nuevos datos)\n",
    "\n",
    "# Guardar datos normalizados\n",
    "df_normalizado.to_csv('../datos/datos_normalizados.csv', index=False)\n",
    "print(\"‚úÖ Datos normalizados guardados: datos/datos_normalizados.csv\")\n",
    "\n",
    "# Guardar datos con PCA\n",
    "df_pca.to_csv('../datos/datos_pca.csv', index=False)\n",
    "print(\"‚úÖ Datos PCA guardados: datos/datos_pca.csv\")\n",
    "\n",
    "# Guardar datos procesados (sin normalizar) para an√°lisis posterior\n",
    "df.to_csv('../datos/datos_procesados.csv', index=False)\n",
    "print(\"‚úÖ Datos procesados guardados: datos/datos_procesados.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15224a",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Resumen del Preprocesamiento\n",
    "\n",
    "### ‚úÖ Acciones Realizadas:\n",
    "\n",
    "1. **Eliminaci√≥n de columnas**: CUST_ID removido\n",
    "2. **Valores nulos**: Imputados con la mediana\n",
    "3. **Duplicados**: Eliminados (si exist√≠an)\n",
    "4. **Outliers**: Mantenidos (importantes para clustering)\n",
    "5. **Normalizaci√≥n**: StandardScaler aplicado (media=0, std=1)\n",
    "6. **PCA**: Reducci√≥n a componentes que explican 80%+ de varianza\n",
    "\n",
    "### üìä Resultado Final:\n",
    "\n",
    "- **Registros limpios**: ~8,950 clientes\n",
    "- **Variables normalizadas**: 17 features\n",
    "- **Componentes PCA**: Variables reducidas\n",
    "- **Datos listos para**: Clustering K-Means\n",
    "\n",
    "### üìÅ Archivos Generados:\n",
    "\n",
    "- `datos_procesados.csv` ‚Üí Datos limpios sin normalizar\n",
    "- `datos_normalizados.csv` ‚Üí Datos listos para clustering\n",
    "- `datos_pca.csv` ‚Üí Datos con PCA para visualizaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "**Pr√≥ximo paso**: Notebook 3 - Clustering con K-Means\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
